{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Now, that I've run some basic counts analyses on the Bible text, I want to extract all of the people names and geopolitical entities contained within its text. I am interested in how many times each person or nation/city/people group is mentioned. In a later project, I am also going to use these lists to scrape the web for any articles related to Biblical archeology and a specific person or nation/city/people group. As such, I will save these list as tables in our SQL database for later use.\n",
    "\n",
    "NOTE: This notebook outputs a lot of full dataframes that I used to determine next steps. I am going to clear these results when I upload to Github because I feel it is too distracting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "This is my typical set up. I import the modules I will use, set my project directory, remove column and row limits, and allow Jupyter to display all of the output from each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "\n",
    "# Set project folder as directory\n",
    "os.chdir(r'C:/Users/david/Projects/Bible Analytics')\n",
    "\n",
    "# Remove row and column limits\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all output from each cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'Data/SQL database.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(database)\n",
    " \n",
    "df = pd.read_sql_query('SELECT * FROM t_web', conn)\n",
    " \n",
    "conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin\n",
    "\n",
    "First, I will define an nlp object by loading a large, English language, trained pipeline. I'm using the large pipeline because I want my model to pick up on as many people names and geopolitical entities as possible. Next, I define two empty dictionaries, one for poeple and one for geopolitical entities. Then I begin iterating through the dataframe into which I will store the results of my search. After this, there is a bit of code used for timing the process. This is more for my personal interest and can be ignored. Next, I will loop through each row of data using a FOR loop. \n",
    "\n",
    "The very first thing I apply is a TRY block. There are some rows of data that contain an empty cell for clean text, which will through back an error and stop the code. This occurs because some of the verses were not contained within the earliest manuscripts. In these cases, this translation places the entire verse inside a set of curly brackets. When we cleaned the data in the last project, we removed anything within curly brackets from the text, which left nothing in these cells of data. However, we still have these verses within the unclean (Bible joke :)) text, so I am going to use an EXCEPT block to capture any information in those verses.\n",
    "\n",
    "Within the TRY BLOCK, I apply our nlp object to the clean text data. This nlp object will break the text into individual tokens and predict which tokens are entities. It will also predict the type of entity. Using these predictions and a nested FOR loop, I will loop through each predicted entity in each line of clean data and search for people or geopolitical entities. Next, when an entity is a person, this code will check if that person's name is already listed as a key in our people dictionary. If so, the value associated with this person's key will be increased by 1. If not, this person's name will be added to the people dictionary as a key and assigned a value of 1. The exact same process will be applied to geopolitical entities.\n",
    "\n",
    "After this bit of code, I start the EXCEPT block and within this EXCEPT block, I am going to initiate another TRY block. The only difference in this block and the TRY block describe above is that I will apply the nlp object to the unclean text. Then I will end with an nexted EXCEPT that sends back a messge to check out a particual verse if it fails to execute in either TRY block.\n",
    "\n",
    "I end with some timing stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "people = {}\n",
    "gpe = {}\n",
    "\n",
    "# Ignore this\n",
    "start = datetime.now()\n",
    "# Stop ignoring\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        doc = nlp(row['clean_t'])\n",
    "    \n",
    "        for ent in doc.ents:\n",
    "            \n",
    "            # People        \n",
    "            if ent.label_ == 'PERSON':\n",
    "                \n",
    "                if ent.text in people:\n",
    "                    people[ent.text]+=1\n",
    "                    \n",
    "                else:\n",
    "                    people[ent.text]=1\n",
    "                    \n",
    "            # GPE        \n",
    "            if ent.label_ == 'GPE':\n",
    "                \n",
    "                if ent.text in gpe:\n",
    "                    gpe[ent.text]+=1\n",
    "                    \n",
    "                else:\n",
    "                    gpe[ent.text]=1\n",
    "                    \n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            doc = nlp(row['t'])\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                \n",
    "                # People        \n",
    "                if ent.label_ == 'PERSON':\n",
    "                    \n",
    "                    if ent.text in people:\n",
    "                        people[ent.text]+=1\n",
    "                        \n",
    "                    else:\n",
    "                        people[ent.text]=1\n",
    "                        \n",
    "                # GPE        \n",
    "                if ent.label_ == 'GPE':\n",
    "                    \n",
    "                    if ent.text in gpe:\n",
    "                        gpe[ent.text]+=1\n",
    "                        \n",
    "                    else:\n",
    "                        gpe[ent.text]=1\n",
    "                    \n",
    "        except:\n",
    "            \n",
    "            print('Check out this verse.')\n",
    "            print(row['name'], row['c'], ':', row['v'])\n",
    "            print()\n",
    "            print()\n",
    "    \n",
    "    \n",
    "# Ignore this\n",
    "stop = datetime.now()\n",
    "\n",
    "print('This process took', stop-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process took a little less than four minutes and did not kick back any errors. That's great! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataframes\n",
    "\n",
    "Before I even look at these results, I know that I want to store them as a SQL table and that dictionaries are not the easiest objects to navigate. As such, I am going to convert both dictionaries into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = pd.DataFrame(zip(people.keys(), people.values()), columns = ['people', 'mentioned'])\n",
    "people_df.sort_values(['people'], ascending=True, inplace=True)\n",
    "people_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_df = pd.DataFrame(zip(gpe.keys(), gpe.values()), columns = ['gpe', 'mentioned'])\n",
    "gpe_df.sort_values(['gpe'], ascending=True, inplace=True)\n",
    "gpe_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "First, let's examine the data to see how good of a job our code did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting God, Jesus, and Moses\n",
    "\n",
    "Right away, I noticed that the name \"Jesus\" was not captured as a person entity. This pipe line was trained on the modern English language and not Biblical text. My guess is that the name \"Jesus\" in modern English is more likely to be an expletive than a reference to the actual, historical person. The same is true of \"God.\" To handle this I will loop through the text again and specifically search for \"Jesus\" and \"God.\" There's got to be a preacher joke in there somewhere! Maybe something about \"and ye shall find.\" Oddly, Moses also only shows up as a phrase \"Moses Zipporah.\" I can't come up with an explanation for Moses not being a person entity that this model picks up.\n",
    "\n",
    "I'm going to loop through the text one time and count the number of times each of these people are mentioned. The first thing I do is define an nlp object by loading the large pipeline. I then define three count variables set to 0. Then I add some code for timing this process. Next I use a FOR loop to iterate through the Bible dataframe. For each row of data I begin with a TRY block. Within that block I create doc by applying the nlp object to the clean data in that row. I then use a nested FOR loop to iterate through each word (token) in the text. Within this nested FOR loop, I use the conditional statements, IF and ELIF, to determine if each token is \"God\", \"Jesus\", or \"Moses\". I picked this order because my hypothesis is that God will show up most, then Jesus, and finally Moses. I didn't have to use the ELIF statements for this. I could have just as easily used IF because the ELIF statements will only save me a fraction of CPU. However, I figured why not and just did it. In other situations, ELIF can save quite a bit of CPU and in this situation, it doesn't hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "jesus = 0\n",
    "god = 0\n",
    "moses = 0\n",
    "\n",
    "# Ignore this\n",
    "start = datetime.now()\n",
    "# Stop ignoring\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        doc = nlp(row['clean_t'])\n",
    "    \n",
    "        for token in doc:\n",
    "            \n",
    "            if token.text == 'God':\n",
    "                god+=1\n",
    "            elif token.text == 'Jesus':\n",
    "                jesus+=1\n",
    "            elif token.text == 'Moses':\n",
    "                moses+=1\n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            doc = nlp(row['t'])\n",
    "            \n",
    "            for token in doc:\n",
    "                \n",
    "                if token.text == 'Jesus':\n",
    "                    jesus+=1\n",
    "                elif token.text == 'God':\n",
    "                    god+=1\n",
    "                elif token.text == 'Moses':\n",
    "                    moses+=1\n",
    "            \n",
    "                    \n",
    "        except:\n",
    "            \n",
    "            print('Check out this verse.')\n",
    "            print(row['name'], row['c'], ':', row['v'])\n",
    "            print()\n",
    "            print()\n",
    "    \n",
    "# Ignore this\n",
    "stop = datetime.now()\n",
    "\n",
    "print('This process took', stop-start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "god\n",
    "jesus\n",
    "moses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, God was mentioned most in the Bible, followed by Jesus and then Moses. I will add these to our people dictionary next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = {'God': god, 'Jesus': jesus, 'Moses': moses}\n",
    "people.update(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['God']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning people dictionary\n",
    "I also need to clean the people list I already have. For instance, \"Aaron\" is the first entry and the second is \"Aaron eighty-three years old,\" which is clearly a phrase about the same guy. I want to get rid of the second entry, but I want to add its count to the first.\n",
    "\n",
    "Many of the incorrect data are phrases which contain names. To extract the names from these phrases, I will search for any words in these phrases that begin with an uppercase letter. This is not a perfect solution because I will also capture other proper nouns such as location names. Additionally, any first word in a sentance will be captured. However, I'm willing to accept this level of noise for now.\n",
    "\n",
    "Below is my code. I have already initialized an nlp object earlier in this notebook, so I don't need to do that again. I use a FOR loop to iterate through the people dataframe and filter to entries that are longer than one word. I then print that entry for later review. Next, I define doc by applying our nlp object to each entry. \n",
    "\n",
    "After this, I create my first nested FOR loop to iterate through each token in each entry. Within this nested FOR loop, I create a variable called upper and set it's initial value to False. I then create another nested FOR loop to iterate through the letters of that word (ele) and use an IF statement to determine whether or not each letter is uppercase. If any element within a word is uppercase, the value of upper is changed to True. I then use a break statement to end the search because it doesn't matter if we find multiple uppercase letters in a word. Upper has already been set to true.\n",
    "\n",
    "Next, we will modify people_dict with the new information. When this code pulls an uppercase word from any of these rows, it will check the people dictionary to determine if this uppercase word is already a key. If so, the value for that key will be increased by one. If not, a new key will be added to the people dictionary with a value of one. \n",
    "\n",
    "Last, I am adding a line of code to remove each phrase from the people dictionary after its content has been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in people_df.iterrows():\n",
    "    \n",
    "    if len(row['people'].split()) > 1:\n",
    "        \n",
    "        print(row['people'])\n",
    "        \n",
    "        doc = nlp(row['people'])\n",
    "        \n",
    "        for token in doc:\n",
    "            \n",
    "            upper = False\n",
    "            \n",
    "            for ele in token.text:\n",
    "                \n",
    "                if ele.isupper():\n",
    "                    \n",
    "                    upper = True\n",
    "                    \n",
    "                    break\n",
    "            \n",
    "            if upper:\n",
    "                \n",
    "                print(token.text)\n",
    "                \n",
    "                if token.text in people:\n",
    "                    people[token.text]+=1\n",
    "                    \n",
    "                else:\n",
    "                    people[token.text]=1\n",
    "                    \n",
    "        # Remove entry for people dictionary                    \n",
    "        people.pop(row['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df2 = pd.DataFrame(zip(people.keys(), people.values()), columns = ['people', 'mentioned'])\n",
    "people_df2.sort_values(['people'], ascending=True, inplace=True)\n",
    "people_df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now three more counts added to \"Aaron\" and the phrases containing Aaron's name is gone. I pretty happy with this. However, I cannot stress enough that this is not a perfect list. There are certainly entries and counts in this list that came from the names of locations and there are also common words that have been included. \n",
    "\n",
    "That said, I think this code did a really great job capturing all of the people in the Bible, and I'm excited analyze this list. One last step, I'm going to remove lowercase words and any one letter words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df2 = people_df2[people_df2['people']!=people_df2['people'].str.lower()]\n",
    "\n",
    "people_df2 = people_df2[people_df2['people'].str.len()>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the analysis begin!!\n",
    "\n",
    "Let's start with rankings. Who are the most mentioned people in the Bible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df2.sort_values(['mentioned'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df2.sort_values(['people'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise, God in all his forms is the most mentioned person in the Bible. We have \"God\" in first place with just over four thousand mentions, \"Yahwah\" with a little over two thousand mentions and \"Jesus\" with just under one-thousand mentions. This does not count all of the times that God is mentioned in the New Testament as the \"Father\", but I think we still get a clear picture. This book is about God!\n",
    "\n",
    "Next, we have Moses. I'm still surprised and confused about why Moses did not show up as a PERSON entity in our search. The model almost certainly would have predicted \"Moses\" as a name based on the way it used in the sentence structure, so there must be a decision rule that explicitly tells the model not to count Moses as an entity. I can't think of why this would be the case. This name is still used in common English as a given name and I can't think of any other way it is used. Perhaps, one day we'll figure this out.\n",
    "\n",
    "The next most commonly mentioned person in the Bible is David, whom I was named after. Then Moses' brother, \n",
    "Aaron.\n",
    "\n",
    "I was actually surprised that Joseph came in seventh with 240 mentions just ahead of his great-grandfather, Abraham, who only had 222 mentions. I may confirm this with another counts analysis. However, Abraham also gets 63 mentions as Abram, so technically Abraham comes in just ahead of Joseph with 285 combined mentions. \n",
    "\n",
    "Joshua, Moses' apprentice, comes in ninth, and Joseph's brother, Benjamin, is tenth. This made me wonder why we didn't see Israel's name in the top ten. Both Israel and Jacob show up but with low counts. I don't know why this would be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geopolitical Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list looks better to me. Of course there is still some noise, but not nearly as much as I saw in the people list. I'm going to keep this list as is and look at the top ten most mentioned gpe in the Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_df.sort_values(['mentioned'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, looks like a pretty solid list. The top ten are exactly what I would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Pandas dataframes to SQL tables\n",
    "\n",
    "As mentioned earlier in this post, I would like to use the name of people and geopolitical entities in a webcrawler to extract articles about Biblical artifacts directly related to specific entities within the text of the Bible. Ultimately, I'd like to compile a dynamic list of these articles and tie them directly to the relevant Biblical text.\n",
    "\n",
    "As such, I am going to save these lists as SQL tables that I can access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(database)\n",
    "\n",
    "people_df2.to_sql('people_names', conn, if_exists='replace', index=False)\n",
    "\n",
    "gpe_df.to_sql('gpe_name', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *table* means double quotes around table\n",
    " \n",
    "conn = sqlite3.connect(database)\n",
    "cursor = conn.cursor()\n",
    " \n",
    "cursor.execute('SELECT name FROM sqlite_master WHERE type=\"table\"')\n",
    " \n",
    "for i in cursor.fetchall():\n",
    "    print(i[0])\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
